{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Project 2\n",
    "\n",
    "## Lecture Transcript Word Frequency Analysis\n",
    "\n",
    "## 100 Points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TB\n",
    "\n",
    "EPT[\"1.1<br>extract_pdf_text\"]\n",
    "RH[\"1.2<br>remove_headers\"]\n",
    "RP[\"1.3<br>remove_parentheses\"]\n",
    "RS[\"1.4<br>remove_speakers\"]\n",
    "\n",
    "SIS[\"2.1<br>split_into_sentences\"]\n",
    "TS[\"2.2<br>tokenize_sentence\"]\n",
    "RSW[\"2.3<br>remove_stopwords\"]\n",
    "\n",
    "SAC[\"3.1<br>save_as_csv\"]\n",
    "\n",
    "CWF[\"4.1<br>update_word_frequency\"]\n",
    "PWFD[\"4.2<br>plot_word_frequency_dist\"]\n",
    "SWBF[\"4.3<br>sort_words_by_frequency\"]\n",
    "FBIOW[\"4.4<br>find_bucket_index_of_words\"]\n",
    "SAJ[\"4.5<br>save_as_json\"]\n",
    "PSWF[\"4.6<br>plot_stacked_word_frequency\"]\n",
    "\n",
    "EPT --> RH\n",
    "RH --> RP\n",
    "RP --> RS\n",
    "\n",
    "RS --> SIS\n",
    "SIS --> TS\n",
    "TS --> RSW\n",
    "\n",
    "RSW --> SAC\n",
    "\n",
    "RSW --> CWF\n",
    "CWF --> PWFD\n",
    "CWF --> SWBF\n",
    "SWBF --> FBIOW\n",
    "\n",
    "FBIOW --> SAJ\n",
    "\n",
    "FBIOW --> PSWF\n",
    "CWF --> PSWF\n",
    "\n",
    "subgraph \"Extract Text Contents\"\n",
    "    EPT\n",
    "    RH\n",
    "    RP\n",
    "    RS\n",
    "end\n",
    "\n",
    "subgraph \"Natural Language Preprocessing\"\n",
    "    SIS\n",
    "    TS\n",
    "    RSW\n",
    "end\n",
    "\n",
    "subgraph \"Save the Cleaned Text\"\n",
    "    SAC\n",
    "end\n",
    "\n",
    "subgraph \"Word Frequency Analysis\"\n",
    "    CWF\n",
    "    PWFD\n",
    "    SWBF\n",
    "    FBIOW\n",
    "    SAJ\n",
    "    PSWF\n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (30 pts): Extract text contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 (5 pts): Extract the text from a PDF file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `extract_pdf_text(file_path)` that:\n",
    "\n",
    "- **Input:** A string `file_path` specifying the path to a PDF file.\n",
    "- **Output:** A string containing all the text extracted from the PDF file.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- For this task, you may use the [`pypdf` module](https://github.com/py-pdf/pypdf) to extract text from the PDF file.\n",
    "  - To install the module, open a terminal and run\n",
    "  ```bash\n",
    "  pip install pypdf\n",
    "  ```\n",
    "  If you are using Linux or MacOS, you may need to run `pip3` instead of `pip`.\n",
    "- Join the text from different pages with a space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file as a string\n",
    "    IN: file_path, str, path to the PDF file\n",
    "    OUT: text, str, extracted text from all pages joined together with whitespaces\n",
    "    \"\"\"\n",
    "    # create a reader to read the PDF file\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    text_list = []  # list to collect text from each page\n",
    "    for page in reader.pages:\n",
    "        # extract text from the page\n",
    "        page_text = page.extract_text()\n",
    "        # append the text to the list\n",
    "        text_list.append(page_text)\n",
    "\n",
    "    # join the text from all pages with whitespaces\n",
    "    text = ' '.join(text_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 (7 pts): Remove headers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `remove_headers(text)` that:\n",
    "\n",
    "- **Input:** A string `text` extracted from the PDF file.\n",
    "- **Output:** A string that does not contain the header texts.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Some lecture transcripts may contain a header with the service name \"StreamBox\" on the first page. Remove it if it is present. See `transcript 1.pdf` for an example.\n",
    "- Some lecture transcripts may start with a Disclaimer. Remove all the text enclosed by `**********DISCLAIMER**********` (included). See `transcript 4.pdf` for an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_headers(text):\n",
    "    \"\"\"\n",
    "    Removes headers from the text\n",
    "    IN: text, str, text to remove headers from\n",
    "    OUT: text, str, text with headers removed\n",
    "    \"\"\"\n",
    "    # if there is any leading \"StreamBox\", remove it\n",
    "    text = text.replace('StreamBox', '')\n",
    "\n",
    "    # if there is any **********DISCLAIMER**********, find the matched end, and remove it\n",
    "    disclaimer_start = text.find('**********DISCLAIMER**********')\n",
    "    if disclaimer_start != -1:\n",
    "        disclaimer_end = text.find('**********', disclaimer_start + 30)\n",
    "        text = text[:disclaimer_start] + text[disclaimer_end + 10:]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 (10 pts): Remove transcription system notes enclosed by parentheses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `remove_parentheses(text)` that:\n",
    "\n",
    "- **Input:** A string `text` that has been processed by the previous functions.\n",
    "- **Output:** A string that does not contain the transcription system notes enclosed by parentheses.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Some lecture transcripts use parentheses to provide additional information about the transcription system. Remove all the text enclosed by parentheses. For example, `[Captioner standing by]` in `transcript 1.pdf`.\n",
    "- There are NO nested parentheses in the lecture transcripts.\n",
    "- The parentheses may either be round parentheses `()` or square parentheses `[]`.\n",
    "- Some parentheses may be followed immediately by a period. For example, `(Away From Mic).` in `transcript 4.pdf`. In this case, you should remove the period as well.\n",
    "- You are welcome to use regular expressions to solve this task, but it is not required. You can use Python built-in string methods to achieve the same result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # if you don't know regular expressions, ignore this import, and use Python plain vanilla string methods\n",
    "\n",
    "\n",
    "def remove_parentheses(text):\n",
    "    \"\"\"\n",
    "    Removes text within parentheses\n",
    "    IN: text, str, text to remove notes enclosed in parentheses\n",
    "    OUT: text, str, text without parentheses notes\n",
    "    \"\"\"\n",
    "    # remove round parentheses () and the text within them. If there is a trailing period, remove it as well\n",
    "    text = re.sub(r'\\(.*?\\)\\.?', '', text)\n",
    "\n",
    "    # remove square parentheses [] and the text within them. If there is a trailing period, remove it as well\n",
    "    text = re.sub(r'\\[.*?\\]\\.?', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 (8 pts): Remove speaker notations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `remove_speakers(text)` that:\n",
    "\n",
    "- **Input:** A string `text` that has been processed by the previous functions.\n",
    "- **Output:** A string that does not contain the speaker notations.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Speaker notations start with two greater-than symbols `>>`, followed by the speaker's name in uppercase letters, and end with a colon `:`. For example, `>> INSTRUCTOR:`.\n",
    "- `>>` symbols are used only for speaker notations and NOT for any other purposes.\n",
    "- You are welcome to use regular expressions to solve this task, but it is not required. You can use Python built-in string methods to achieve the same result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # if you don't know regular expressions, ignore this import, and use Python plain vanilla string methods\n",
    "\n",
    "\n",
    "def remove_speakers(text):\n",
    "    \"\"\"\n",
    "    Removes speaker names from the text\n",
    "    IN: text, str, text to remove speaker notations from\n",
    "    OUT: text, str, text without speaker notations\n",
    "    \"\"\"\n",
    "    # remove speaker notations\n",
    "    text = re.sub(r'[A-Z][A-Z]+:', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (20 pts): Natural Language Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you may use a Natural Language Processing module [`spaCy`](https://spacy.io/) to preprocess the text data.\n",
    "\n",
    "**Installation Instructions:**\n",
    "\n",
    "1. To install the spaCy module, open a terminal and run:\n",
    "   ```bash\n",
    "   pip install spacy\n",
    "   python -m spacy download en_core_web_sm\n",
    "   ```\n",
    "   If you are using Linux or MacOS, you may need to run `pip3` instead of `pip`, and `python3` instead of `python`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 (6 pts): Split text into sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `split_into_sentences(text)` that\n",
    "\n",
    "- **Input:** A string `text` that has been processed by the previous functions.\n",
    "- **Output:** A list of strings, where each string is a sentence.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- `spaCy` provides a sentence segmentation method that can be used to split the text into sentences. Below is an example of how to implement this functionality:\n",
    "\n",
    "  ```python\n",
    "  import spacy\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\") # Load the English language model\n",
    "\n",
    "  text = \"This is a sentence. This is another sentence.\"\n",
    "  doc = nlp(text) # Process the input text\n",
    "\n",
    "  # Extract sentences from the processed document\n",
    "  sentences = [sent.text for sent in doc.sents]\n",
    "  print(sentences)\n",
    "  ```\n",
    "\n",
    "  The printed output will be\n",
    "\n",
    "  ```python\n",
    "  ['This is a sentence.', 'This is another sentence.']\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_into_sentences\u001b[39m(text):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits text into sentences\n",
    "    IN: text, str, text to split into sentences\n",
    "    OUT: sentences, list[str], sentences in the text\n",
    "    \"\"\"\n",
    "    # join lines into a single paragraph\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # remove leading and trailing whitespaces in the text\n",
    "    text = text.strip()\n",
    "\n",
    "    # use spaCy to split the text into sentences\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 (8 pts): Tokenize sentences and remove punctuations and spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `tokenize_sentence(sentence)` that\n",
    "\n",
    "- **Input:** A string `sentence` that has been processed by the previous functions.\n",
    "- **Output:** A list of strings, where each string is a lemmatized token in the sentence.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- You can use the `spaCy` module to tokenize the sentence and remove punctuation and spaces. Below is an example demonstrating how to implement this functionality:\n",
    "\n",
    "  ```python\n",
    "  import spacy\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\") # Load the English language model\n",
    "\n",
    "  sentence = \"This is a sentence.\"\n",
    "  doc = nlp(sentence) # Process the input sentence\n",
    "\n",
    "  # Extract lemmatized tokens, excluding punctuation and spaces\n",
    "  tokens = [\n",
    "    token.lemma_\n",
    "    for token in doc\n",
    "    if not token.is_punct and not token.is_space\n",
    "  ]\n",
    "  print(tokens)\n",
    "  ```\n",
    "\n",
    "  The printed output will be\n",
    "\n",
    "  ```python\n",
    "  ['this', 'be', 'a', 'sentence']\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_sentence\u001b[39m(sentence):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes sentence into lemmatized words\n",
    "    IN: sentence, str, sentence to tokenize\n",
    "    OUT: tokenized_sentence, list[str], tokenized sentences\n",
    "    \"\"\"\n",
    "    # use spaCy to tokenize the sentence into lemmatized words\n",
    "    doc = nlp(sentence)\n",
    "    tokenized_sentence = [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 (6 pts): Remove stopwords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `remove_stopwords(tokens)` that\n",
    "\n",
    "- **Input:** A list of strings `tokens` that have been processed by the previous functions.\n",
    "- **Output:** A filtered list of strings that does not contain stopwords.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- You can use the `spaCy` module to remove the stopwords. Below is an example demonstrating how to implement this functionality, including adding an additional stopword \"blah\":\n",
    "\n",
    "  ```python\n",
    "  import spacy\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\") # Load the English language model\n",
    "\n",
    "  nlp.Defaults.stop_words.add(\"blah\") # Add an additional stopword \"blah\"\n",
    "\n",
    "  tokens = [\"this\", \"be\", \"a\", \"sentence\"]\n",
    "\n",
    "  # Filter out stopwords from the tokens list\n",
    "  tokens = [token for token in tokens if not nlp.vocab[token].is_stop]\n",
    "  print(tokens)\n",
    "  ```\n",
    "\n",
    "  The printed output will be\n",
    "\n",
    "  ```python\n",
    "  ['sentence']\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m nlp\u001b[39m.\u001b[39mDefaults\u001b[39m.\u001b[39mstop_words\u001b[39m.\u001b[39madd(\u001b[39m\"\u001b[39m\u001b[39mblah\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.Defaults.stop_words.add(\"blah\")\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Removes stopwords from tokens\n",
    "    IN: tokens, list[str], tokens to remove stopwords from\n",
    "    OUT: tokens, list[str], tokens with stopwords removed\n",
    "    \"\"\"\n",
    "    # use spaCy to remove stopwords\n",
    "    tokens = [token for token in tokens if not nlp.vocab[token].is_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (5 pts): Save the cleaned text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 (5 pts): Save the processed text as a csv file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `save_as_csv(process_text, file_path)` that\n",
    "\n",
    "- **Input 1:** A list of lists of strings `processed_text`, where each inner list contains the processed tokens of a sentence in a lecture transcript.\n",
    "- **Input 2:** A string `file_path` that specifies the path to save the CSV file.\n",
    "- **Write** the processed text to a CSV file at the specified `file_path`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def save_as_csv(process_text, file_path):\n",
    "    \"\"\"\n",
    "    Saves processed text to a CSV file\n",
    "    IN: process_text, list[list[str]], processed text of a lecture transcript to save to a CSV file\n",
    "        file_path, str, path to the CSV file\n",
    "    OUT: None\n",
    "    \"\"\"\n",
    "    # write the processed text to a CSV file\n",
    "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for sentence_tokens in process_text:\n",
    "            writer.writerow(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (45 pts): Word Frequency Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 (10 pts): Compute the cumulative word frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `update_word_frequency(processed_text, cumulated_frequency)` that\n",
    "\n",
    "- **Input 1:** A list of lists of strings `processed_text`, where each inner list contains the processed tokens of a sentence in a lecture transcript.\n",
    "- **Input 2:** A dictionary `cumulated_frequency` that stores the cumulated word frequency up to now. By default, `cumulated_frequency` is an empty dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't know Counter, ignore this import. Recall Counter is a subclass of dict\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def update_word_frequency(processed_text, cumulated_frequency=None):\n",
    "    \"\"\"\n",
    "    Updates the cumulated word frequency with the frequency in the processed text\n",
    "    IN: processed_text, list[list[str]], processed text to count word frequency\n",
    "        cumulated_frequency, dict{str: int}, cumulated word frequency\n",
    "    OUT: cumulated_frequency, dict{str: int}, cumulated word frequency\n",
    "    \"\"\"\n",
    "    # if cumulated_frequency is not provided, initialize it\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # update the cumulated frequency with the frequency in each sentence\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return cumulated_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 (5 pts): Plot the word frequency as a histogram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `plot_word_frequency_dist(cumulated_frequency)` that\n",
    "\n",
    "- **Input:** A dictionary `cumulated_frequency` that stores the cumulated word frequency.\n",
    "- Plot the word frequency as a histogram.\n",
    "\n",
    "Note:\n",
    "\n",
    "- This function plots a histogram where the x-axis represents the word frequency, and the y-axis represents the number of words that appear with the corresponding frequency.\n",
    "- You may use the `matplotlib` module to create the histogram. If you haven't installed it yet, you can do so by running the following command in your terminal:\n",
    "  ```bash\n",
    "  pip install matplotlib\n",
    "  ```\n",
    "- To plot a histogram, you may use the plt.hist() function, which takes a list of values as input.\n",
    "\n",
    "  ```python\n",
    "  import matplotlib.pyplot as plt\n",
    "  values = [1, 1, 1, 2, 3, 3, 3, 3, 3, 4, 4, 5]\n",
    "  plt.hist(values)\n",
    "  plt.show()\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_word_frequency_dist(cumulated_frequency):\n",
    "    \"\"\"\n",
    "    Plots word frequency\n",
    "    IN: cumulated_frequency, dict{str: int}, cumulated word frequency\n",
    "    OUT: None\n",
    "    \"\"\"\n",
    "    # plot the word frequency distribution\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    plt.xlabel(\"Word Frequency\")\n",
    "    plt.ylabel(\"Num of Words\")\n",
    "    plt.title(\"Word Frequency Distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 (5 pts): Sort the words by frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `sort_words_by_frequency(cumulated_frequency)` that\n",
    "\n",
    "- **Input:** A dictionary `cumulated_frequency` that stores the cumulated word frequency.\n",
    "- **Output:** A list of tuples, where each tuple contains a word and its frequency, sorted by frequency in non-ascending order. If two words have the same frequency, they are sorted in alphabetical non-descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_words_by_frequency(cumulated_frequency):\n",
    "    \"\"\"\n",
    "    Sorts words by frequency\n",
    "    IN: cumulated_frequency, dict{str: int}, cumulated word frequency\n",
    "    OUT: sorted_word_frequencies, list[tuple(str, int)], sorted words by frequency\n",
    "    \"\"\"\n",
    "    # sort the words by frequency, breaking ties by word alphabetical order\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 (10 pts): Bucket the words by frequency using mean and standard deviation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 (3 pts): Find the mean and standard deviation of the word frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `find_mean_and_std(sorted_word_frequencies)` that\n",
    "\n",
    "- **Input:** A sorted list of tuples `sorted_word_frequencies` consisting of words and their frequencies.\n",
    "- **Output:** A tuple `(mean, std)` where `mean` is the average of the word frequencies and `std` is the standard deviation of the word frequencies.\n",
    "\n",
    "#### Formulas:\n",
    "\n",
    "- Mean:\n",
    "\n",
    "  $$\n",
    "  \\bar{f} = \\frac{\\sum_{i=1}^{n} f_i}{n}\n",
    "  $$\n",
    "\n",
    "  where $f_i$ is the frequency of the $i$-th word and $n$ is the number of words.\n",
    "\n",
    "- Standard Deviation:\n",
    "  $$\n",
    "  \\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n} (f_i - \\bar{f})^2}{n}}\n",
    "  $$\n",
    "  where $f_i$ is the frequency of the $i$-th word, $\\bar{f}$ is the mean of the word frequency, and $n$ is the number of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_and_std(sorted_word_frequencies):\n",
    "    \"\"\"\n",
    "    Finds mean and standard deviation of word frequency\n",
    "    IN: sorted_word_frequencies, list[tuple(str, int)], sorted words by frequency\n",
    "    OUT: mean, float, mean of word frequency\n",
    "         std, float, standard deviation of word frequency\n",
    "    \"\"\"\n",
    "    # access all the frequencies\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # calculate the mean\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # calculate the standard deviation\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.2 (3 pts): Bucket a value by the mean and standard deviation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `bucket_value_by_mean_std(value, mean, std)` that\n",
    "\n",
    "- **Input 1:** A number `value` representing the frequency of a word.\n",
    "- **Input 2:** A float `mean` representing the mean of the word frequency.\n",
    "- **Input 3:** A float `std` representing the standard deviation of the word frequency.\n",
    "- **Output:** An integer representing the bucket index of the word frequency.\n",
    "\n",
    "**Formula:**\n",
    "The bucket index can be calculated using the formula:\n",
    "\n",
    "$$\n",
    "b = \\left\\lfloor \\frac{f - \\bar{f}}{\\sigma} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $f$ is the frequency of the word,\n",
    "- $\\bar{f}$ is the mean of the word frequency,\n",
    "- $\\sigma$ is the standard deviation of the word frequency,\n",
    "- $\\lfloor \\cdot \\rfloor$ is the floor function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def bucket_value_by_mean_std(value, mean, std):\n",
    "    \"\"\"\n",
    "    Buckets value by mean and standard deviation\n",
    "    IN: value, int | float, value from a distribution to bucket\n",
    "        mean, float, mean of the distribution\n",
    "        std, float, standard deviation of the distribution\n",
    "    OUT: bucket_idx, int, bucket index\n",
    "    \"\"\"\n",
    "    # calculate the bucket index\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return bucket_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.3 (4 pts): Find the bucket index of each word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `find_bucket_index_of_words(sorted_word_frequencies)` that\n",
    "\n",
    "- **Input:** A sorted list of tuples `sorted_word_frequencies` consisting of words and their frequencies.\n",
    "- **Output:** A list of dictionaries, where each dictionary contains three key-value pairs:\n",
    "  - `word`: the word from the input list.\n",
    "  - `frequency`: the frequency of the word.\n",
    "  - `bucket`: the bucket index of the word frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bucket_index_of_words(sorted_word_frequencies):\n",
    "    \"\"\"\n",
    "    Finds bucket index of words by mean and standard deviation\n",
    "    IN: sorted_word_frequencies, list[tuple(str, int)], sorted words by frequency\n",
    "    OUT: sorted_word_info, list[dict{\"word\": str, \"frequency\": int, \"bucket\": int}], words with frequency and bucket index\n",
    "    \"\"\"\n",
    "    # find the mean and standard deviation of the word frequencies\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # collect the words with their frequency and bucket index\n",
    "    sorted_word_info = []\n",
    "    for word, freq in sorted_word_frequencies:\n",
    "        \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return sorted_word_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 (5 pts): Save the word information as a JSON file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `save_as_json(sorted word_info, file_path)` that\n",
    "\n",
    "- **Input 1:** A sorted list of dictionaries `sorted_word_info` that contains frequency and bucket information of each word.\n",
    "- **Input 2:** A string `file_path` that specifies the path to save the JSON file.\n",
    "- **Write** the word information as a JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def save_as_json(sorted_word_info, file_path):\n",
    "    \"\"\"\n",
    "    Saves word info to a JSON file\n",
    "    IN: sorted_word_info, list[dict{\"word\": str, \"frequency\": int, \"bucket\": int}], words with frequency and bucket index\n",
    "        file_path, str, path to the JSON file\n",
    "    OUT: None\n",
    "    \"\"\"\n",
    "    # write the word info to a JSON file\n",
    "    \"\"\"__Your_Code_Here__\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 (10 pts): Plot stacked word frequency bar chart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `plot_stacked_word_frequency(word_frequencies_list, sorted_words)` that\n",
    "\n",
    "- **Input 1:** A list of dictionaries `word_frequencies_list`. Each dictionary contains the cumulated word frequency up to a certain lecture transcript.\n",
    "\n",
    "  The first dictionary contains the word frequency of the first lecture transcript, the second dictionary contains the cumulated word frequency of the first and second lecture transcripts, and so on.\n",
    "\n",
    "- **Input 2:** A list of tuples `sorted_words` that contains the words sorted by frequency.\n",
    "- **Plot** the word frequency as a stacked bar chart.\n",
    "\n",
    "Note:\n",
    "\n",
    "- A stacked bar chart uses bars to show comparisons between categories of data, with the ability to break down and compare parts of a whole. In this task:\n",
    "  - The x-axis represents the words.\n",
    "  - The y-axis represents the word frequency.\n",
    "  - Bars are sorted using the order of `sorted_words`.\n",
    "- You may use the `matplotlib` module to plot the stacked bar chart. The following code snippet demonstrates how to create a stacked bar chart using the `plt.bar()` function.\n",
    "\n",
    "  ```python\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  cmap = plt.get_cmap(\"viridis_r\")  # use colormap for coloring the bars\n",
    "\n",
    "  words = [\"apple\", \"banana\", \"cherry\"]  # list all words used in x-axis\n",
    "  frequencies_list = [  # list of cumulated word frequencies\n",
    "      [1, 3, 0],  # frequencies in document 1\n",
    "      [3, 3, 3],  # frequencies in document 1 and 2\n",
    "      [6, 5, 3],  # frequencies in document 1, 2, and 3\n",
    "  ]\n",
    "\n",
    "  for i, frequencies in reversed(list(enumerate(frequencies_list))):  # plot in reverse order, so the earlier one is on top\n",
    "      plt.bar(\n",
    "          words, # x-axis, words\n",
    "          frequencies, # y-axis, cumulated frequencies\n",
    "          color=cmap(i / (len(frequencies_list) - 1)), # color the bars\n",
    "          label=f\"Up to Document {i+1}\", # label the part of the bar\n",
    "      )\n",
    "\n",
    "  plt.xlabel(\"Words\")\n",
    "  plt.xticks(rotation=90) # rotate the x-axis labels for better readability\n",
    "  plt.ylabel(\"Frequency\")\n",
    "  plt.title(\"Word Frequency in Documents\")\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_stacked_word_frequency(word_frequencies_list, sorted_words):\n",
    "    \"\"\"\n",
    "    Plots stacked word frequency\n",
    "    IN: word_frequencies_list, list[dict{str: int}], word frequencies to plot\n",
    "        sorted_words, list[tuple(str, int)], sorted words by frequency\n",
    "    OUT: None\n",
    "    \"\"\"\n",
    "    # set up color map\n",
    "    cmp = plt.get_cmap(\"viridis_r\")\n",
    "\n",
    "    # get words and frequencies\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # plot the stacked word frequency\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Stacked Word Frequency\")\n",
    "    # move the legend to the right to avoid overlapping with the bars\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call of functions for testing purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # this block of code is executed when directly running the script, not when importing the script as a module\n",
    "    \"\"\"\n",
    "    Feel free to modify the code inside this if block to test your functions\n",
    "    \"\"\"\n",
    "    word_frequencies_list = []\n",
    "    cumulated_frequency = None\n",
    "    for i in range(1, 13):\n",
    "        fileName = f'./data/transcript {i}.pdf'\n",
    "        print(f\"Processing {fileName}\")\n",
    "\n",
    "        # extract text from the PDF file\n",
    "        text = extract_pdf_text(fileName)\n",
    "        text = remove_headers(text)\n",
    "        text = remove_parentheses(text)\n",
    "        text = remove_speakers(text)\n",
    "\n",
    "        # natural language preprocessing\n",
    "        sentences = split_into_sentences(text)\n",
    "        processed_text = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_sentence(sentence)\n",
    "            tokens = remove_stopwords(tokens)\n",
    "            processed_text.append(tokens)\n",
    "\n",
    "        # update count of word frequency\n",
    "        cumulated_frequency = update_word_frequency(\n",
    "            processed_text, cumulated_frequency)\n",
    "        word_frequencies_list.append(cumulated_frequency.copy())\n",
    "\n",
    "    # plot the word final cumulative frequency distribution\n",
    "    plot_word_frequency_dist(word_frequencies_list[-1])\n",
    "\n",
    "    # sort words by final cumulative frequency\n",
    "    sorted_words = sort_words_by_frequency(word_frequencies_list[-1])\n",
    "\n",
    "    # find bucket index of all words\n",
    "    sorted_word_info = find_bucket_index_of_words(sorted_words)\n",
    "\n",
    "    # take words whose bucket index >= 5 (i.e. frequency >= mean + 5 * std)\n",
    "    filtered_words = [(info['word'], info['frequency'])\n",
    "                      for info in sorted_word_info if info['bucket'] >= 5]\n",
    "\n",
    "    # plot stacked word frequency\n",
    "    plot_stacked_word_frequency(word_frequencies_list, filtered_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
