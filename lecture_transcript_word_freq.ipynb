{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Project 2\n",
    "\n",
    "## Lecture Transcript Word Frequency Analysis\n",
    "\n",
    "## 100 Points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TB\n",
    "    \n",
    "EPT[\"1.1<br>extract_pdf_text\"]\n",
    "RH[\"1.2<br>remove_headers\"]\n",
    "RP[\"1.3<br>remove_parentheses\"]\n",
    "RS[\"1.4<br>remove_speakers\"]\n",
    "\n",
    "SIS[\"2.1<br>split_into_sentences\"]\n",
    "TS[\"2.2<br>tokenize_sentence\"]\n",
    "RSW[\"2.3<br>remove_stopwords\"]\n",
    "\n",
    "SAC[\"3.1<br>save_as_csv\"]\n",
    "\n",
    "CWF[\"4.1<br>update_word_frequency\"]\n",
    "PWFD[\"4.2<br>plot_word_frequency_dist\"]\n",
    "SWBF[\"4.3<br>sort_words_by_frequency\"]\n",
    "FBIOW[\"4.4<br>find_bucket_index_of_words\"]\n",
    "SAJ[\"4.5<br>save_as_json\"]\n",
    "PSWF[\"4.6<br>plot_stacked_word_frequency\"]\n",
    "\n",
    "EPT --> RH\n",
    "RH --> RP\n",
    "RP --> RS\n",
    "\n",
    "RS --> SIS\n",
    "SIS --> TS\n",
    "TS --> RSW\n",
    "\n",
    "RSW --> SAC\n",
    "\n",
    "RSW --> CWF\n",
    "CWF --> PWFD\n",
    "CWF --> SWBF\n",
    "SWBF --> FBIOW\n",
    "\n",
    "FBIOW --> SAJ\n",
    "\n",
    "FBIOW --> PSWF\n",
    "CWF --> PSWF\n",
    "\n",
    "subgraph \"Extract Text Contents\"\n",
    "    EPT\n",
    "    RH\n",
    "    RP\n",
    "    RS\n",
    "end\n",
    "\n",
    "subgraph \"Natural Language Preprocessing\"\n",
    "    SIS\n",
    "    TS\n",
    "    RSW\n",
    "end\n",
    "\n",
    "subgraph \"Save the Cleaned Text\"\n",
    "    SAC\n",
    "end\n",
    "\n",
    "subgraph \"Word Frequency Analysis\"\n",
    "    CWF\n",
    "    PWFD\n",
    "    SWBF\n",
    "    FBIOW\n",
    "    SAJ\n",
    "    PSWF\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (30 pts): Extract text contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 (5 pts): Extract the text from a PDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `extract_pdf_text(file_path)` that:\n",
    "\n",
    "- **Input:** A string `file_path` specifying the path to a PDF file.\n",
    "- **Output:** A string containing all the text extracted from the PDF file.\n",
    "\n",
    "**Note:**\n",
    "- For this task, you may use the [`pypdf` module](https://github.com/py-pdf/pypdf) to extract text from the PDF file.\n",
    "  - To install the module, open a terminal and run \n",
    "  ```bash\n",
    "  pip install pypdf\n",
    "  ```\n",
    "  If you are using Linux or MacOS, you may need to run `pip3` instead of `pip`.\n",
    "- Join the text from different pages with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file as a string\n",
    "    IN: file_path, str, path to the PDF file\n",
    "    OUT: text, str, extracted text from all pages joined together with whitespaces\n",
    "    \"\"\"\n",
    "    # create a reader to read the PDF file\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    text_list = [] # list to collect text from each page\n",
    "    for page in reader.pages:\n",
    "        # extract text from the page\n",
    "        page_text = page.extract_text() \n",
    "        # append the text to the list\n",
    "        \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # join the text from all pages with whitespaces\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 (7 pts): Remove headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `remove_headers(text)` that:\n",
    "\n",
    "- **Input:** A string `text` extracted from the PDF file.\n",
    "- **Output:** A string that does not contain the header texts.\n",
    "\n",
    "**Note:**\n",
    "- Some lecture transcripts may contain a header with the service name \"StreamBox\" on the first page. Remove it if it is present. See `transcript 1.pdf` for an example.\n",
    "- Some lecture transcripts may start with a Disclaimer. Remove all the text enclosed by `**********DISCLAIMER**********` (included). See `transcript 4.pdf` for an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_headers(text):\n",
    "    \"\"\"\n",
    "    Removes headers from the text\n",
    "    IN: text, str, text to remove headers from\n",
    "    OUT: text, str, text with headers removed\n",
    "    \"\"\"\n",
    "    # if there is any leading \"StreamBox\", remove it\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # if there is any **********DISCLAIMER**********, find the matched end, and remove it\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 (10 pts): Remove transcription system notes enclosed by parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `remove_parentheses(text)` that:\n",
    "\n",
    "- **Input:** A string `text` that has been processed by the previous functions.\n",
    "- **Output:** A string that does not contain the transcription system notes enclosed by parentheses.\n",
    "\n",
    "**Note:**\n",
    "- Some lecture transcripts use parentheses to provide additional information about the transcription system. Remove all the text enclosed by parentheses. For example, `[Captioner standing by]` in `transcript 1.pdf`.\n",
    "- There are NO nested parentheses in the lecture transcripts.\n",
    "- The parentheses may either be round parentheses `()` or square parentheses `[]`.\n",
    "- Some parentheses may be followed immediately by a period. For example, `(Away From Mic).` in `transcript 4.pdf`. In this case, you should remove the period as well.\n",
    "- You are welcome to use regular expressions to solve this task, but it is not required. You can use Python built-in string methods to achieve the same result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # if you don't know regular expressions, ignore this import, and use Python plain vanilla string methods\n",
    "\n",
    "def remove_parentheses(text):\n",
    "    \"\"\"\n",
    "    Removes text within parentheses\n",
    "    IN: text, str, text to remove notes enclosed in parentheses\n",
    "    OUT: text, str, text without parentheses notes\n",
    "    \"\"\"\n",
    "    # remove round parentheses () and the text within them. If there is a trailing period, remove it as well\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # remove square parentheses [] and the text within them. If there is a trailing period, remove it as well\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 (8 pts): Remove speaker notations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `remove_speakers(text)` that:\n",
    "\n",
    "- **Input:** A string `text` that has been processed by the previous functions.\n",
    "- **Output:** A string that does not contain the speaker notations.\n",
    "\n",
    "**Note:**\n",
    "- Speaker notations start with two greater-than symbols `>>`, followed by the speaker's name in uppercase letters, and end with a colon `:`. For example, `>> INSTRUCTOR:`.\n",
    "- `>>` symbols are used only for speaker notations and NOT for any other purposes.\n",
    "- You are welcome to use regular expressions to solve this task, but it is not required. You can use Python built-in string methods to achieve the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # if you don't know regular expressions, ignore this import, and use Python plain vanilla string methods\n",
    "\n",
    "def remove_speakers(text):\n",
    "    \"\"\"\n",
    "    Removes speaker names from the text\n",
    "    IN: text, str, text to remove speaker notations from\n",
    "    OUT: text, str, text without speaker notations\n",
    "    \"\"\"\n",
    "    # remove speaker notations\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (20 pts): Natural Language Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you may use a Natural Language Processing module [`spaCy`](https://spacy.io/) to preprocess the text data.\n",
    "\n",
    "**Installation Instructions:**\n",
    "1. To install the spaCy module, open a terminal and run:\n",
    "   ```bash\n",
    "   pip install spacy\n",
    "   python -m spacy download en_core_web_sm\n",
    "    ```\n",
    "    If you are using Linux or MacOS, you may need to run `pip3` instead of `pip`, and `python3` instead of `python`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 (6 pts): Split text into sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `split_into_sentences(text)` that\n",
    "\n",
    "- **Input:** A string `text` that has been processed by the previous functions.\n",
    "- **Output:** A list of strings, where each string is a sentence.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- `spaCy` provides a sentence segmentation method that can be used to split the text into sentences. Below is an example of how to implement this functionality:\n",
    "\n",
    "  ```python\n",
    "  import spacy\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\") # Load the English language model\n",
    "\n",
    "  text = \"This is a sentence. This is another sentence.\"\n",
    "  doc = nlp(text) # Process the input text\n",
    "\n",
    "  # Extract sentences from the processed document\n",
    "  sentences = [sent.text for sent in doc.sents]\n",
    "  print(sentences)\n",
    "  ```\n",
    "  The printed output will be \n",
    "  ```python\n",
    "  ['This is a sentence.', 'This is another sentence.']\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits text into sentences\n",
    "    IN: text, str, text to split into sentences\n",
    "    OUT: sentences, list[str], sentences in the text\n",
    "    \"\"\"\n",
    "    # join lines into a single paragraph\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # remove leading and trailing whitespaces in the text\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # use spaCy to split the text into sentences\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 (8 pts): Tokenize sentences and remove punctuations and spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `tokenize_sentence(sentence)` that\n",
    "\n",
    "- **Input:** A string `sentence` that has been processed by the previous functions.\n",
    "- **Output:** A list of strings, where each string is a lemmatized token in the sentence.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- You can use the `spaCy` module to tokenize the sentence and remove punctuation and spaces. Below is an example demonstrating how to implement this functionality:\n",
    "\n",
    "  ```python\n",
    "  import spacy\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\") # Load the English language model\n",
    "  \n",
    "  sentence = \"This is a sentence.\"\n",
    "  doc = nlp(sentence) # Process the input sentence\n",
    "\n",
    "  # Extract lemmatized tokens, excluding punctuation and spaces\n",
    "  tokens = [\n",
    "    token.lemma_ \n",
    "    for token in doc \n",
    "    if not token.is_punct and not token.is_space\n",
    "  ]\n",
    "  print(tokens)\n",
    "  ```\n",
    "  The printed output will be \n",
    "  ```python\n",
    "  ['this', 'be', 'a', 'sentence']\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes sentence into lemmatized words\n",
    "    IN: sentence, str, sentence to tokenize\n",
    "    OUT: tokenized_sentence, list[str], tokenized sentences\n",
    "    \"\"\"\n",
    "    # use spaCy to tokenize the sentence into lemmatized words\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 (6 pts): Remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `remove_stopwords(tokens)` that\n",
    "\n",
    "- **Input:** A list of strings `tokens` that have been processed by the previous functions.\n",
    "- **Output:** A filtered list of strings that does not contain stopwords.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- You can use the `spaCy` module to remove the stopwords. Below is an example demonstrating how to implement this functionality, including adding an additional stopword \"blah\":\n",
    "\n",
    "  ```python\n",
    "  import spacy\n",
    "\n",
    "  nlp = spacy.load(\"en_core_web_sm\") # Load the English language model\n",
    "\n",
    "  nlp.Defaults.stop_words.add(\"blah\") # Add an additional stopword \"blah\"\n",
    "\n",
    "  tokens = [\"this\", \"be\", \"a\", \"sentence\"]\n",
    "\n",
    "  # Filter out stopwords from the tokens list\n",
    "  tokens = [token for token in tokens if not nlp.vocab[token].is_stop]\n",
    "  print(tokens)\n",
    "  ```\n",
    "  The printed output will be \n",
    "  ```python\n",
    "  ['sentence']\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.Defaults.stop_words.add(\"blah\")\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Removes stopwords from tokens\n",
    "    IN: tokens, list[str], tokens to remove stopwords from\n",
    "    OUT: tokens, list[str], tokens with stopwords removed\n",
    "    \"\"\"\n",
    "    # use spaCy to remove stopwords\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (5 pts): Save the cleaned text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 (5 pts): Save the processed text as a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `save_as_csv(process_text, file_path)` that\n",
    "\n",
    "- **Input 1:** A list of lists of strings `processed_text`, where each inner list contains the processed tokens of a sentence in a lecture transcript.\n",
    "- **Input 2:** A string `file_path` that specifies the path to save the CSV file.\n",
    "- **Write** the processed text to a CSV file at the specified `file_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_as_csv(process_text, file_path):\n",
    "    \"\"\"\n",
    "    Saves processed text to a CSV file\n",
    "    IN: process_text, list[list[str]], processed text of a lecture transcript to save to a CSV file\n",
    "        file_path, str, path to the CSV file\n",
    "    OUT: None\n",
    "    \"\"\"\n",
    "    # write the processed text to a CSV file\n",
    "    \"\"\"__Your_Code_Here__\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (45 pts): Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 (10 pts): Compute the cumulative word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `update_word_frequency(processed_text, cumulated_frequency)` that\n",
    "\n",
    "- **Input 1:** A list of lists of strings `processed_text`, where each inner list contains the processed tokens of a sentence in a lecture transcript.\n",
    "- **Input 2:** A dictionary `cumulated_frequency` that stores the cumulated word frequency up to now. By default, `cumulated_frequency` is an empty dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # if you don't know Counter, ignore this import. Recall Counter is a subclass of dict\n",
    "\n",
    "def update_word_frequency(processed_text, cumulated_frequency=None):\n",
    "    \"\"\"\n",
    "    Updates the cumulated word frequency with the frequency in the processed text\n",
    "    IN: processed_text, list[list[str]], processed text to count word frequency\n",
    "        cumulated_frequency, dict{str: int}, cumulated word frequency\n",
    "    OUT: cumulated_frequency, dict{str: int}, cumulated word frequency\n",
    "    \"\"\"\n",
    "    # if cumulated_frequency is not provided, initialize it\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # update the cumulated frequency with the frequency in each sentence\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return cumulated_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 (5 pts): Plot the word frequency as a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `plot_word_frequency_dist(cumulated_frequency)` that\n",
    "\n",
    "- **Input:** A dictionary `cumulated_frequency` that stores the cumulated word frequency.\n",
    "- Plot the word frequency as a histogram.\n",
    "\n",
    "Note:\n",
    "\n",
    "- This function plots a histogram where the x-axis represents the word frequency, and the y-axis represents the number of words that appear with the corresponding frequency.\n",
    "- You may use the `matplotlib` module to create the histogram. If you haven't installed it yet, you can do so by running the following command in your terminal:\n",
    "  ```bash\n",
    "  pip install matplotlib\n",
    "  ```\n",
    "- To plot a histogram, you may use the plt.hist() function, which takes a list of values as input.\n",
    "\n",
    "  ```python\n",
    "  import matplotlib.pyplot as plt\n",
    "  values = [1, 1, 1, 2, 3, 3, 3, 3, 3, 4, 4, 5]\n",
    "  plt.hist(values)\n",
    "  plt.show()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_frequency_dist(cumulated_frequency):\n",
    "    \"\"\"\n",
    "    Plots word frequency\n",
    "    IN: cumulated_frequency, dict{str: int}, cumulated word frequency\n",
    "    OUT: None\n",
    "    \"\"\"\n",
    "    # plot the word frequency distribution\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    \n",
    "\n",
    "    plt.xlabel(\"Word Frequency\")\n",
    "    plt.ylabel(\"Num of Words\")\n",
    "    plt.title(\"Word Frequency Distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 (5 pts): Sort the words by frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `sort_words_by_frequency(cumulated_frequency)` that\n",
    "\n",
    "- **Input:** A dictionary `cumulated_frequency` that stores the cumulated word frequency.\n",
    "- **Output:** A list of tuples, where each tuple contains a word and its frequency, sorted by frequency in non-ascending order. If two words have the same frequency, they are sorted in alphabetical non-descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_words_by_frequency(cumulated_frequency):\n",
    "    \"\"\"\n",
    "    Sorts words by frequency\n",
    "    IN: cumulated_frequency, dict{str: int}, cumulated word frequency\n",
    "    OUT: sorted_word_frequencies, list[tuple(str, int)], sorted words by frequency\n",
    "    \"\"\"\n",
    "    # sort the words by frequency, breaking ties by word alphabetical order\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 (10 pts): Bucket the words by frequency using mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 (3 pts): Find the mean and standard deviation of the word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `find_mean_and_std(sorted_word_frequencies)` that\n",
    "\n",
    "- **Input:** A sorted list of tuples `sorted_word_frequencies` consisting of words and their frequencies.\n",
    "- **Output:** A tuple `(mean, std)` where `mean` is the average of the word frequencies and `std` is the standard deviation of the word frequencies.\n",
    "\n",
    "#### Formulas:\n",
    "- Mean: \n",
    "  $$\n",
    "  \\bar{f} = \\frac{\\sum_{i=1}^{n} f_i}{n}\n",
    "  $$\n",
    "  where $f_i$ is the frequency of the $i$-th word and $n$ is the number of words.\n",
    "\n",
    "- Standard Deviation: \n",
    "  $$\n",
    "  \\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n} (f_i - \\bar{f})^2}{n}}\n",
    "  $$\n",
    "  where $f_i$ is the frequency of the $i$-th word, $\\bar{f}$ is the mean of the word frequency, and $n$ is the number of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_and_std(sorted_word_frequencies):\n",
    "    \"\"\"\n",
    "    Finds mean and standard deviation of word frequency\n",
    "    IN: sorted_word_frequencies, list[tuple(str, int)], sorted words by frequency\n",
    "    OUT: mean, float, mean of word frequency\n",
    "         std, float, standard deviation of word frequency\n",
    "    \"\"\"\n",
    "    # access all the frequencies\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # calculate the mean\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # calculate the standard deviation\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.2 (3 pts): Bucket a value by the mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `bucket_value_by_mean_std(value, mean, std)` that\n",
    "\n",
    "- **Input 1:** A number `value` representing the frequency of a word.\n",
    "- **Input 2:** A float `mean` representing the mean of the word frequency.\n",
    "- **Input 3:** A float `std` representing the standard deviation of the word frequency.\n",
    "- **Output:** An integer representing the bucket index of the word frequency.\n",
    "\n",
    "**Formula:**\n",
    "The bucket index can be calculated using the formula:\n",
    "$$\n",
    "b = \\left\\lfloor \\frac{f - \\bar{f}}{\\sigma} \\right\\rfloor\n",
    "$$\n",
    "where:\n",
    "- $f$ is the frequency of the word,\n",
    "- $\\bar{f}$ is the mean of the word frequency,\n",
    "- $\\sigma$ is the standard deviation of the word frequency,\n",
    "- $\\lfloor \\cdot \\rfloor$ is the floor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def bucket_value_by_mean_std(value, mean, std):\n",
    "    \"\"\"\n",
    "    Buckets value by mean and standard deviation\n",
    "    IN: value, int | float, value from a distribution to bucket\n",
    "        mean, float, mean of the distribution\n",
    "        std, float, standard deviation of the distribution\n",
    "    OUT: bucket_idx, int, bucket index\n",
    "    \"\"\"\n",
    "    # calculate the bucket index\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    \n",
    "    return bucket_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.3 (4 pts): Find the bucket index of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `find_bucket_index_of_words(sorted_word_frequencies)` that\n",
    "\n",
    "- **Input:** A sorted list of tuples `sorted_word_frequencies` consisting of words and their frequencies.\n",
    "- **Output:** A list of dictionaries, where each dictionary contains three key-value pairs:\n",
    "  - `word`: the word from the input list.\n",
    "  - `frequency`: the frequency of the word.\n",
    "  - `bucket`: the bucket index of the word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bucket_index_of_words(sorted_word_frequencies):\n",
    "    \"\"\"\n",
    "    Finds bucket index of words by mean and standard deviation\n",
    "    IN: sorted_word_frequencies, list[tuple(str, int)], sorted words by frequency\n",
    "    OUT: sorted_word_info, list[dict{\"word\": str, \"frequency\": int, \"bucket\": int}], words with frequency and bucket index\n",
    "    \"\"\"\n",
    "    # find the mean and standard deviation of the word frequencies\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # collect the words with their frequency and bucket index\n",
    "    sorted_word_info = []\n",
    "    for word, freq in sorted_word_frequencies:\n",
    "        \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    return sorted_word_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 (5 pts): Save the word information as a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `save_as_json(sorted word_info, file_path)` that\n",
    "\n",
    "- **Input 1:** A sorted list of dictionaries `sorted_word_info` that contains frequency and bucket information of each word.\n",
    "- **Input 2:** A string `file_path` that specifies the path to save the JSON file.\n",
    "- **Write** the word information as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_as_json(sorted_word_info, file_path):\n",
    "    \"\"\"\n",
    "    Saves word info to a JSON file\n",
    "    IN: sorted_word_info, list[dict{\"word\": str, \"frequency\": int, \"bucket\": int}], words with frequency and bucket index\n",
    "        file_path, str, path to the JSON file\n",
    "    OUT: None\n",
    "    \"\"\"\n",
    "    # write the word info to a JSON file\n",
    "    \"\"\"__Your_Code_Here__\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 (10 pts): Plot stacked word frequency bar chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `plot_stacked_word_frequency(word_frequencies_list, sorted_words)` that\n",
    "\n",
    "- **Input 1:** A list of dictionaries `word_frequencies_list`. Each dictionary contains the cumulated word frequency up to a certain lecture transcript. \n",
    "  \n",
    "  The first dictionary contains the word frequency of the first lecture transcript, the second dictionary contains the cumulated word frequency of the first and second lecture transcripts, and so on.\n",
    "- **Input 2:** A list of tuples `sorted_words` that contains the words sorted by frequency.\n",
    "- **Plot** the word frequency as a stacked bar chart.\n",
    "\n",
    "Note:\n",
    "\n",
    "- A stacked bar chart uses bars to show comparisons between categories of data, with the ability to break down and compare parts of a whole. In this task:\n",
    "  - The x-axis represents the words.\n",
    "  - The y-axis represents the word frequency.\n",
    "  - Bars are sorted using the order of `sorted_words`.\n",
    "- You may use the `matplotlib` module to plot the stacked bar chart. The following code snippet demonstrates how to create a stacked bar chart using the `plt.bar()` function.\n",
    "\n",
    "  ```python\n",
    "  import matplotlib.pyplot as plt\n",
    "  \n",
    "  cmap = plt.get_cmap(\"viridis_r\")  # use colormap for coloring the bars\n",
    "  \n",
    "  words = [\"apple\", \"banana\", \"cherry\"]  # list all words used in x-axis\n",
    "  frequencies_list = [  # list of cumulated word frequencies\n",
    "      [1, 3, 0],  # frequencies in document 1\n",
    "      [3, 3, 3],  # frequencies in document 1 and 2\n",
    "      [6, 5, 3],  # frequencies in document 1, 2, and 3\n",
    "  ]\n",
    "  \n",
    "  for i, frequencies in reversed(list(enumerate(frequencies_list))):  # plot in reverse order, so the earlier one is on top\n",
    "      plt.bar(\n",
    "          words, # x-axis, words\n",
    "          frequencies, # y-axis, cumulated frequencies\n",
    "          color=cmap(i / (len(frequencies_list) - 1)), # color the bars\n",
    "          label=f\"Up to Document {i+1}\", # label the part of the bar\n",
    "      )\n",
    "  \n",
    "  plt.xlabel(\"Words\")\n",
    "  plt.xticks(rotation=90) # rotate the x-axis labels for better readability\n",
    "  plt.ylabel(\"Frequency\")\n",
    "  plt.title(\"Word Frequency in Documents\")\n",
    "  plt.legend()\n",
    "  \n",
    "  plt.show()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stacked_word_frequency(word_frequencies_list, sorted_words):\n",
    "    \"\"\"\n",
    "    Plots stacked word frequency\n",
    "    IN: word_frequencies_list, list[dict{str: int}], word frequencies to plot\n",
    "        sorted_words, list[tuple(str, int)], sorted words by frequency\n",
    "    OUT: None\n",
    "    \"\"\"\n",
    "    # set up color map\n",
    "    cmp = plt.get_cmap(\"viridis_r\")\n",
    "\n",
    "    # get words and frequencies\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    # plot the stacked word frequency\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Stacked Word Frequency\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5)) # move the legend to the right to avoid overlapping with the bars\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call of functions for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # this block of code is executed when directly running the script, not when importing the script as a module\n",
    "    \"\"\"\n",
    "    Feel free to modify the code inside this if block to test your functions\n",
    "    \"\"\"\n",
    "    word_frequencies_list = []\n",
    "    cumulated_frequency = None\n",
    "    for i in range(1, 13):\n",
    "        fileName = f'./data/transcript {i}.pdf'\n",
    "        print(f\"Processing {fileName}\")\n",
    "\n",
    "        # extract text from the PDF file\n",
    "        text = extract_pdf_text(fileName)\n",
    "        text = remove_headers(text)\n",
    "        text = remove_parentheses(text)\n",
    "        text = remove_speakers(text)\n",
    "\n",
    "        # natural language preprocessing\n",
    "        sentences = split_into_sentences(text)\n",
    "        processed_text = []\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenize_sentence(sentence)\n",
    "            tokens = remove_stopwords(tokens)\n",
    "            processed_text.append(tokens)\n",
    "\n",
    "        # update count of word frequency\n",
    "        cumulated_frequency = update_word_frequency(processed_text, cumulated_frequency)\n",
    "        word_frequencies_list.append(cumulated_frequency.copy())\n",
    "\n",
    "    # plot the word final cumulative frequency distribution\n",
    "    plot_word_frequency_dist(word_frequencies_list[-1])\n",
    "\n",
    "    # sort words by final cumulative frequency\n",
    "    sorted_words = sort_words_by_frequency(word_frequencies_list[-1])\n",
    "\n",
    "    # find bucket index of all words\n",
    "    sorted_word_info = find_bucket_index_of_words(sorted_words)\n",
    "\n",
    "    # take words whose bucket index >= 5 (i.e. frequency >= mean + 5 * std)\n",
    "    filtered_words = [(info['word'], info['frequency']) for info in sorted_word_info if info['bucket'] >= 5]\n",
    "\n",
    "    # plot stacked word frequency\n",
    "    plot_stacked_word_frequency(word_frequencies_list, filtered_words)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
